{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ghWaLRFSGR"
      },
      "source": [
        "# Patch pooling (10pts)\n",
        "Your task is to implement patch pooling. Patch pooling takes an input sequence $(a_0, a_1, \\ldots, a_{B-1})$ of $D$-dimensional embeddings and output an output sequence $(b_0, \\ldots, b_{k-1})$ of $D$-dimensional embeddings. The length of the output sequence is not longer than the length of the input sequence and is bounded by $P$. Each element of the input sequence is called a token. Each element of the output sequence is called a patch. Consecutive patches are constructed as a mean pooling of consecutive contiguous token spans.\n",
        "\n",
        "You are given two tensors:\n",
        "1. `batch` - a $3$-dimensional tensor, which is an input to a standard transformer model with the following dimensions:\n",
        "* B - batch size\n",
        "* S - sequence lenght\n",
        "* D - dimension of embedding of a single token\n",
        "\n",
        "`batch[x,y,:]` is the embedding of the $y+1$-th token of the $x+1$-th sequence in the `batch`.\n",
        "\n",
        "2. `patch_lengths` - $2$-dimensional integer-valued tensor with the following dimensions:\n",
        "* B - batch size\n",
        "* P - maximal number of patches\n",
        "\n",
        "`patch_lengths[x,y]` is the number of tokens forming patch number $y+1$ in the $x+1$-th sequence in the `batch`.\n",
        "\n",
        "The output should be a $3$-dimensional tensor with batch of sequences of patch embeddings.\n",
        "\n",
        "# Example\n",
        "The following snippet\n",
        "```python\n",
        "batch = torch.tensor([[[ 1.,  1.,  1.,  1.,  1.],\n",
        "         [ 1.,  1.,  1.,  1.,  1.],\n",
        "         [ 1.,  1.,  1.,  1.,  1.],\n",
        "         [ 2.,  2.,  2.,  2.,  2.],\n",
        "         [ 3.,  3.,  3.,  3.,  3.],\n",
        "         [ 3.,  3.,  3.,  3.,  3.]],\n",
        "\n",
        "        [[ 4.,  4.,  4.,  4.,  4.],\n",
        "         [ 4.,  4.,  4.,  4.,  4.],\n",
        "         [ 4.,  4.,  4.,  4.,  4.],\n",
        "         [ 4.,  4.,  4.,  4.,  4.],\n",
        "         [ 5.,  5.,  5.,  5.,  5.],\n",
        "         [-1., -1., -1., -1., -1.]],\n",
        "\n",
        "        [[ 6.,  6.,  6.,  6.,  6.],\n",
        "         [-1., -1., -1., -1., -1.],\n",
        "         [-1., -1., -1., -1., -1.],\n",
        "         [-1., -1., -1., -1., -1.],\n",
        "         [-1., -1., -1., -1., -1.],\n",
        "         [-1., -1., -1., -1., -1.]]])\n",
        "patch_lengths = torch.tensor([[3, 1, 2],\n",
        "        [4, 1, 0],\n",
        "        [1, 0, 0]])\n",
        "patch_pooling = PatchPooling()\n",
        "output = patch_pooling(batch, patch_lengths)\n",
        "output\n",
        "```\n",
        "\n",
        "should ouptut\n",
        "\n",
        "```python\n",
        "torch.tensor([[[1., 1., 1., 1., 1.],\n",
        "         [2., 2., 2., 2., 2.],\n",
        "         [3., 3., 3., 3., 3.]],\n",
        "\n",
        "        [[4., 4., 4., 4., 4.],\n",
        "         [5., 5., 5., 5., 5.],\n",
        "         [-1., -1., -1., -1., -1.]],\n",
        "\n",
        "        [[6., 6., 6., 6., 6.],\n",
        "         [-1., -1., -1., -1., -1.],\n",
        "         [-1., -1., -1., -1., -1.]]])\n",
        "```\n",
        "\n",
        "Remarks:\n",
        "\n",
        "1. In this problem you can assume that embeddings of the padding token are vectors with all coordinates equal to $-1$.\n",
        "\n",
        "2. Solutions will be graded with unit tests. You are given a single test case, which will be a part of evaluation.\n",
        "\n",
        "3. Solutions not satisfying the below requirements will be graded up to 4pts:\n",
        "* You are not allowed to call custom python functions\n",
        "* You are not allowed to use Python loops\n",
        "* Your are not allowed to use any other imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NPA3vycFSGV",
        "outputId": "702913f0-a4b6-4bda-8c8c-31bf714317e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_patch_pooling.py\n"
          ]
        }
      ],
      "source": [
        "%%file test_patch_pooling.py\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "class PatchPooling(torch.nn.Module):\n",
        "    def forward(self, batch: torch.Tensor, patch_lengths: torch.Tensor) -> torch.Tensor:\n",
        "        B, S, D = batch.shape\n",
        "        B_1, P = patch_lengths.shape\n",
        "\n",
        "        assert B == B_1\n",
        "\n",
        "        ### Your code goes here ###\n",
        "\n",
        "        # Create output tensor initialized with padding\n",
        "        output = torch.full((B, P, D), -1.0, dtype=batch.dtype, device=batch.device)\n",
        "\n",
        "        # Create cumulative sum of patch_lengths to get start indices for each patch\n",
        "        cumsum = torch.cumsum(patch_lengths, dim=1)\n",
        "        start_indices = torch.cat([torch.zeros(B, 1, dtype=torch.long, device=batch.device), cumsum[:, :-1]], dim=1)\n",
        "\n",
        "        # Create a mask for valid patches (patch_length > 0)\n",
        "        valid_mask = patch_lengths > 0\n",
        "\n",
        "        # Get maximum patch length to create offset indices\n",
        "        max_patch_len = patch_lengths.max().item()\n",
        "\n",
        "        # Create offset indices [0, 1, 2, ..., max_patch_len-1]\n",
        "        offset = torch.arange(max_patch_len, device=batch.device).unsqueeze(0).unsqueeze(0)  # (1, 1, max_patch_len)\n",
        "\n",
        "        # Expand to (B, P, max_patch_len)\n",
        "        start_expanded = start_indices.unsqueeze(2)  # (B, P, 1)\n",
        "        token_indices = start_expanded + offset  # (B, P, max_patch_len)\n",
        "\n",
        "        # Clamp indices to valid range\n",
        "        token_indices = token_indices.clamp(0, S - 1)\n",
        "\n",
        "        # Create mask for valid tokens within each patch\n",
        "        patch_lengths_expanded = patch_lengths.unsqueeze(2)  # (B, P, 1)\n",
        "        token_mask = offset < patch_lengths_expanded  # (B, P, max_patch_len)\n",
        "\n",
        "        # Expand batch indices\n",
        "        batch_indices = torch.arange(B, device=batch.device).view(B, 1, 1).expand(B, P, max_patch_len)\n",
        "\n",
        "        # Gather tokens: batch[batch_indices, token_indices, :]\n",
        "        gathered = batch[batch_indices, token_indices, :]  # (B, P, max_patch_len, D)\n",
        "\n",
        "        # Apply mask and compute mean\n",
        "        token_mask_expanded = token_mask.unsqueeze(3)  # (B, P, max_patch_len, 1)\n",
        "        masked_gathered = gathered * token_mask_expanded  # Zero out invalid tokens\n",
        "\n",
        "        # Sum and divide by patch_length\n",
        "        patch_sums = masked_gathered.sum(dim=2)  # (B, P, D)\n",
        "        patch_lengths_for_div = patch_lengths.unsqueeze(2).clamp(min=1).float()  # (B, P, 1), avoid div by 0\n",
        "        patch_means = patch_sums / patch_lengths_for_div  # (B, P, D)\n",
        "\n",
        "        # Only update valid patches\n",
        "        valid_mask_expanded = valid_mask.unsqueeze(2)  # (B, P, 1)\n",
        "        output = torch.where(valid_mask_expanded, patch_means, output)\n",
        "\n",
        "        ###########################\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TestPatchPooling:\n",
        "    @pytest.mark.parametrize(\n",
        "        \"batch,patch_lengths,expected_output\",\n",
        "        [\n",
        "            (\n",
        "                torch.tensor(\n",
        "                    [\n",
        "                        [\n",
        "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "                            [2.0, 2.0, 2.0, 2.0, 2.0],\n",
        "                            [3.0, 3.0, 3.0, 3.0, 3.0],\n",
        "                            [3.0, 3.0, 3.0, 3.0, 3.0],\n",
        "                        ],\n",
        "                        [\n",
        "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
        "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
        "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
        "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
        "                            [5.0, 5.0, 5.0, 5.0, 5.0],\n",
        "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
        "                        ],\n",
        "                        [\n",
        "                            [6.0, 6.0, 6.0, 6.0, 6.0],\n",
        "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
        "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
        "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
        "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
        "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
        "                        ],\n",
        "                    ]\n",
        "                ),\n",
        "                torch.tensor([[3, 1, 2], [4, 1, 0], [1, 0, 0]]),\n",
        "                torch.tensor(\n",
        "                    [\n",
        "                        [\n",
        "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "                            [2.0, 2.0, 2.0, 2.0, 2.0],\n",
        "                            [3.0, 3.0, 3.0, 3.0, 3.0],\n",
        "                        ],\n",
        "                        [\n",
        "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
        "                            [5.0, 5.0, 5.0, 5.0, 5.0],\n",
        "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
        "                        ],\n",
        "                        [\n",
        "                            [6.0, 6.0, 6.0, 6.0, 6.0],\n",
        "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
        "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
        "                        ],\n",
        "                    ]\n",
        "                ),\n",
        "            )\n",
        "        ],\n",
        "    )\n",
        "    def test_forward(\n",
        "        self,\n",
        "        batch: torch.Tensor,\n",
        "        patch_lengths: torch.Tensor,\n",
        "        expected_output: torch.Tensor,\n",
        "    ) -> None:\n",
        "        # given\n",
        "        patch_pooling = PatchPooling()\n",
        "\n",
        "        # when\n",
        "        output = patch_pooling(batch=batch, patch_lengths=patch_lengths)\n",
        "\n",
        "        # then\n",
        "        assert torch.all(torch.isclose(output, expected_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI__yScxFSGX",
        "outputId": "c9b6e7e2-e8e5-4104-e4f1-e05ce234fd95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.6.8, typeguard-4.4.4, anyio-4.12.1\n",
            "collected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_patch_pooling.py \u001b[32m.\u001b[0m\u001b[32m                                                  [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 3.85s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pytest test_patch_pooling.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw9BVO4VFYki"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}