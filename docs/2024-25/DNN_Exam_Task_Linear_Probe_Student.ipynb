{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Probe\n",
        "\n",
        "#YOU DO NOT NEED GPU FOR THIS TASK, dont worry.\n",
        "\n",
        "In this task, you will implement a training of linear probe.\n",
        "Probing is a technique of checking if model has a specific information encoded in its hidden state. The simplest way to do it is to train a `probe`, different model that gets hidden state of the original model as input and tries to predict a specific information about this input (e.g. is the word currently processed a noun?). The simplest probe is a linear model.\n",
        "In this notebook, we provide implementation of a simple model consisting of\n",
        "\n",
        "1.   Embedding\n",
        "2.   Two attention layers\n",
        "3.   Classification head\n",
        "\n",
        "This model is trained on a specific task:\n",
        "\n",
        "\n",
        "1.   The input to the model is a sequence with exactly one 0 inside (e.g. 1,2,0,4,5,6)\n",
        "2.   The model should predict the first number after 0 (you can assume that 0 is not on the last position) (in the example, it should be 4)\n",
        "3. Output of the model is the output of the classification head on the last number in the sequence (in this case, 6)\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "\n",
        "\n",
        "1.  (2p) Complete the `generate_batch` for the training specified above and run the training (should reach >90 % accuracy with the provided config, you don't need GPU)\n",
        "2.  (2p) Complete the `get_model_embedding_at_positions` and `get_first_attention_output_at_positions` functions for the probe training.\n",
        "3.  (6p) Write the training of the probe (complete `train_probe`). Probe should be trained to answer a task: \"is the token immediately before the current token 0?\" (e.g. in sequence 1, 2, 0, 4, 5, the probe should output number < 0 for \"2\",\"5\" and > 0 for \"4\"). The probe should be a trained using BCEWithLogitsLoss.\n",
        "It is up to you how you will sample negative examples for probe training, but you should sample them in the way that balances the number of positive and negative examples.\n",
        "You should at least log the accuracy during training. Probe training should reach >90% accuracy when training on the output of the first attention, but should stay at ~50-60% when trained on the output of the embedding (as output of embedding has no information of the previous tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a_PFYFWYr4up"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1BxK1Uq-U2c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super(Attention, self).__init__()\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.attention_mechanism = nn.MultiheadAttention(d_model, n_heads, bias=False, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        q = self.w_q(x)\n",
        "        k = self.w_k(x)\n",
        "        v = self.w_v(x)\n",
        "        mask = torch.triu(torch.ones(q.size(1), q.size(1)), diagonal=1).bool().to(x.device)\n",
        "        attention_output, _ = self.attention_mechanism(q, k, v, attn_mask = mask, is_causal=True)\n",
        "        return attention_output + residual\n",
        "\n",
        "class FullEncoding(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, seq_length):\n",
        "        super(FullEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_length = seq_length\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(seq_length, d_model)\n",
        "    def forward(self, x):\n",
        "        token_embedding = self.token_embedding(x)\n",
        "        position_embedding = self.position_embedding(torch.arange(x.size(1)).unsqueeze(0).to(x.device))\n",
        "        return token_embedding + position_embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, seq_length):\n",
        "        super(MultiAttentionModel, self).__init__()\n",
        "        self.full_embedding = FullEncoding(d_model, vocab_size, seq_length)\n",
        "        self.layers = nn.ModuleList([\n",
        "            Attention(d_model, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, x):\n",
        "        x = self.full_embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.head(x)\n",
        "\n",
        "def generate_sequences(vocab_size, batch_size, sequence_length):\n",
        "    return torch.randint(1, vocab_size, (batch_size, sequence_length))\n",
        "\n",
        "def generate_0_placements(batch_size, sequence_length):\n",
        "    return torch.randint(0, sequence_length-1, (batch_size,))\n",
        "\n",
        "def generate_batch(sequences, placements):\n",
        "    \"\"\"\n",
        "    This function, given sequences and placements of 0 tokens, should\n",
        "    generate a batch for MultiAttentionModel\n",
        "    sequences: torch tensor of shape (batch_size, sequence_length)\n",
        "    placements: torch tensor of shape (batch_size,)\n",
        "    return:\n",
        "    sequences_with_0: torch tensor of shape (batch_size, sequence_length), which\n",
        "    is the same as sequences but with 0 at the given placements\n",
        "    targets: torch tensor of shape (batch_size,), which is the number after\n",
        "    0 in the sequences\n",
        "    \"\"\"\n",
        "    batch_size = sequences.size(0)\n",
        "    sequences_with_0 = sequences.clone()\n",
        "    # Task 1\n",
        "    # Place 0 at the specified positions\n",
        "    for i in range(batch_size):\n",
        "        sequences_with_0[i, placements[i]] = 0\n",
        "\n",
        "    # Get the targets - the token immediately after the 0\n",
        "    targets = sequences[torch.arange(batch_size), placements + 1]\n",
        "    # End Task 1\n",
        "\n",
        "    return sequences_with_0, targets\n",
        "\n",
        "example_sequences = torch.tensor(\n",
        "    [\n",
        "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "        [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
        "    ]\n",
        ")\n",
        "example_placements = torch.tensor([2, 7])\n",
        "example_targets = torch.tensor([4, 19])\n",
        "example_sequences_with_0 = torch.tensor(\n",
        "    [\n",
        "        [1, 2, 0, 4, 5, 6, 7, 8, 9, 10],\n",
        "        [11, 12, 13, 14, 15, 16, 17, 0, 19, 20],\n",
        "    ]\n",
        ")\n",
        "assert torch.all(generate_batch(example_sequences, example_placements)[0] == example_sequences_with_0)\n",
        "assert torch.all(generate_batch(example_sequences, example_placements)[1] == example_targets)\n",
        "\n",
        "\n",
        "\n",
        "def train(model, optimizer, batch_size, sequence_length, num_epochs, device):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        sequences = generate_sequences(vocab_size, batch_size, sequence_length)\n",
        "        placements = generate_0_placements(batch_size, sequence_length)\n",
        "        batch, target = generate_batch(sequences, placements)\n",
        "        batch = batch.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        last_token_logits = output[:, -1, :]\n",
        "        loss = criterion(last_token_logits, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "            predicted_token = torch.argmax(last_token_logits, dim=1)\n",
        "            accuracy = (predicted_token == target).float().mean().item()\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "sequence_length = 10\n",
        "vocab_size = 128\n",
        "d_model = 16\n",
        "num_heads = 1\n",
        "num_layers = 2\n",
        "num_epochs = 2000\n",
        "learning_rate = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(0)\n",
        "model = MultiAttentionModel(vocab_size, d_model, num_heads, num_layers, sequence_length)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train(model, optimizer, batch_size, sequence_length, num_epochs, device)\n"
      ],
      "metadata": {
        "id": "KzEXXu5C_S4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_embedding_at_positions(model: MultiAttentionModel, x, positions):\n",
        "    \"\"\"\n",
        "    model: MultiAttentionModel\n",
        "    x: torch tensor of shape (batch_size, sequence_length)\n",
        "    positions: torch tensor of shape (batch_size,)\n",
        "\n",
        "    Example: if model on x has embedding '[[[0, 1], [2, 3], [4, 5]], [[6, 7], [8, 9], [10, 11]]]'\n",
        "    and positions is [0, 2] then the output should be '[[0, 1], [10, 11]]'\n",
        "    return: torch tensor of shape (batch_size, d_model)\n",
        "\n",
        "    \"\"\"\n",
        "    # Task 2.1\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.full_embedding(x)  # (batch_size, sequence_length, d_model)\n",
        "\n",
        "    # Select embeddings at the specified positions for each batch element\n",
        "    batch_indices = torch.arange(x.size(0))\n",
        "    selected_embeddings = embeddings[batch_indices, positions]  # (batch_size, d_model)\n",
        "\n",
        "    return selected_embeddings\n",
        "    # End Task 2.1\n",
        "\n",
        "def get_first_attention_output_at_positions(model, x, positions):\n",
        "    \"\"\"\n",
        "    model: MultiAttentionModel\n",
        "    x: torch tensor of shape (batch_size, sequence_length)\n",
        "    positions: torch tensor of shape (batch_size,)\n",
        "\n",
        "    Example: if first attention of model on x outputs '[[[0, 1], [2, 3], [4, 5]], [[6, 7], [8, 9], [10, 11]]]'\n",
        "    and positions is [0, 2] then the output should be '[[0, 1], [10, 11]]'\n",
        "    return: torch tensor of shape (batch_size, d_model)\n",
        "    \"\"\"\n",
        "    # Task 2.2\n",
        "    with torch.no_grad():\n",
        "        x_embed = model.full_embedding(x)\n",
        "        # Pass through the first attention layer\n",
        "        first_attention_output = model.layers[0](x_embed)  # (batch_size, sequence_length, d_model)\n",
        "\n",
        "    # Select outputs at the specified positions for each batch element\n",
        "    batch_indices = torch.arange(x.size(0))\n",
        "    selected_outputs = first_attention_output[batch_indices, positions]  # (batch_size, d_model)\n",
        "\n",
        "    return selected_outputs\n",
        "    # End Task 2.2\n",
        "\n"
      ],
      "metadata": {
        "id": "g-lfmQeYGeGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_probe(model, probe, optimizer, batch_size, sequence_length, num_epochs, device, model_latent_function):\n",
        "    \"\"\"\n",
        "    Implement the training of the probe.\n",
        "    model_latent_function: either get_model_embedding_at_positions or get_first_attention_output_at_positions\n",
        "    \"\"\"\n",
        "    # Task 3\n",
        "    model = model.to(device)\n",
        "    probe = probe.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        probe.train()\n",
        "\n",
        "        # Generate sequences and placements\n",
        "        sequences = generate_sequences(vocab_size, batch_size, sequence_length)\n",
        "        placements = generate_0_placements(batch_size, sequence_length)\n",
        "        batch, _ = generate_batch(sequences, placements)\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # We need to create balanced positive and negative examples\n",
        "        # Positive examples: positions right after 0 (placements + 1)\n",
        "        # Negative examples: random positions that are NOT right after 0\n",
        "\n",
        "        # Positive examples - tokens immediately after 0\n",
        "        positive_positions = placements + 1  # (batch_size,)\n",
        "        positive_latents = model_latent_function(model, batch, positive_positions)\n",
        "        positive_labels = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "        # Negative examples - sample positions that are not immediately after 0\n",
        "        # We'll sample positions that are at least 2 positions away from the 0\n",
        "        negative_positions = torch.zeros(batch_size, dtype=torch.long)\n",
        "        for i in range(batch_size):\n",
        "            # Available positions are all except placements[i] and placements[i]+1\n",
        "            available_positions = list(range(sequence_length))\n",
        "            # Remove the 0 position and the position right after it\n",
        "            if placements[i].item() in available_positions:\n",
        "                available_positions.remove(placements[i].item())\n",
        "            if (placements[i] + 1).item() in available_positions:\n",
        "                available_positions.remove((placements[i] + 1).item())\n",
        "            # Randomly select one of the available positions\n",
        "            if available_positions:\n",
        "                negative_positions[i] = torch.tensor(available_positions[torch.randint(0, len(available_positions), (1,)).item()])\n",
        "            else:\n",
        "                # Fallback (shouldn't happen with sequence_length=10)\n",
        "                negative_positions[i] = 0\n",
        "\n",
        "        negative_latents = model_latent_function(model, batch, negative_positions)\n",
        "        negative_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Combine positive and negative examples\n",
        "        all_latents = torch.cat([positive_latents, negative_latents], dim=0)\n",
        "        all_labels = torch.cat([positive_labels, negative_labels], dim=0)\n",
        "\n",
        "        # Shuffle the combined data\n",
        "        shuffle_indices = torch.randperm(all_latents.size(0))\n",
        "        all_latents = all_latents[shuffle_indices]\n",
        "        all_labels = all_labels[shuffle_indices]\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        predictions = probe(all_latents)\n",
        "        loss = criterion(predictions, all_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Log accuracy\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            with torch.no_grad():\n",
        "                probe.eval()\n",
        "                predicted_labels = (predictions > 0).float()\n",
        "                accuracy = (predicted_labels == all_labels).float().mean().item()\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # End Task 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7EDE1X93L56G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probe = nn.Linear(d_model, 1)\n",
        "optimizer = torch.optim.Adam(probe.parameters(), lr=learning_rate)\n",
        "\n",
        "train_probe(model, probe, optimizer, batch_size, sequence_length, num_epochs, device, get_model_embedding_at_positions)"
      ],
      "metadata": {
        "id": "bVxz5RKmRlTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probe = nn.Linear(d_model, 1)\n",
        "optimizer = torch.optim.Adam(probe.parameters(), lr=learning_rate)\n",
        "\n",
        "train_probe(model, probe, optimizer, batch_size, sequence_length, num_epochs, device, get_first_attention_output_at_positions)\n"
      ],
      "metadata": {
        "id": "-iwZRHe9RkIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}